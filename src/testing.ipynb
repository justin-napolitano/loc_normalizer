{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6711c0b9-b97e-4eef-93e8-96ad19fc2266",
   "metadata": {},
   "source": [
    "+++\n",
    "title =  \"GCP Cloud Run: LOC Flattener\"\n",
    "date = \"2024-04-28\"\n",
    "description = \"Flattening and injesting JSON into data lake.. Autonomously. \"\n",
    "author = \"Justin Napolitano\"\n",
    "tags = ['git', 'python', 'gcp', 'bash','workflow automation', 'docker','containerization']\n",
    "images = [\"images/feature-gcp.png\"]\n",
    "categories = [\"projects\"]\n",
    "+++\n",
    "\n",
    "\n",
    "# Library of Congress Normalizer Job\n",
    "\n",
    "This [repo](https://github.com/justin-napolitano/loc_normalizer) normalizes the existing library of congress schema into a db that wil then be used to construct a knowledge graph of supreme court law. \n",
    "\n",
    "## Plan\n",
    "\n",
    "1. Setup a venv to run locally\n",
    "2. Install requirements\n",
    "3. Write out the script to interface with gcp\n",
    "4. Set up a docker container and test locally\n",
    "5. build the image\n",
    "6. upload to gcp\n",
    "7. create the job\n",
    "\n",
    "## Setup the venv\n",
    "\n",
    "### Install\n",
    "I installed virtualenv locally on ubuntu\n",
    "\n",
    "### Create\n",
    "I then run ```virtualenv {path to venvs}```\n",
    "\n",
    "### Activate\n",
    "\n",
    "Then source the venv bin to activate\n",
    "\n",
    "```source {path to venv}/bin/activate```\n",
    "   \n",
    "### Install requirements\n",
    "\n",
    "``` pip install -r requirements.txt```\n",
    "\n",
    "## Write out the Script\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Access the loc_scraper Bucket\n",
    "2. Grab a json blob\n",
    "3. Process the blob\n",
    "4. Move the blob to a processed bucket\n",
    "\n",
    "\n",
    "### Data Organization\n",
    "\n",
    "I want to create workflow class with the following methods\n",
    "\n",
    "1. get_creds\n",
    "2. grab_blob\n",
    "3. process_blob\n",
    "4. move_blob\n",
    "\n",
    "The process_blob method will be a lot of work.  I might just flatten the json and dump into a table. I will then write a normalization workflow\n",
    "\n",
    "\n",
    "### Get Creds\n",
    "\n",
    "If running locally I will need some creds in the enviornment. I will take create a key from the console and download it for local run . \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Setup the Docker Container\n",
    "\n",
    "### The Dockerfile\n",
    "\n",
    "Also available on [github](https://github.com/justin-napolitano/loc_normalizer/blob/main/Dockerfile)\n",
    "\n",
    "\n",
    "```\n",
    "# # Use the Alpine Linux base image\n",
    "# FROM alpine:latest\n",
    "\n",
    "# # Set the working directory inside the container\n",
    "# WORKDIR /app\n",
    "\n",
    "# # Copy a simple script that prints \"Hello, World!\" into the container\n",
    "# COPY /src/hello.sh .\n",
    "\n",
    "# # Make the script executable\n",
    "# RUN chmod +x hello.sh\n",
    "\n",
    "# # Define the command to run when the container starts\n",
    "# CMD [\"./hello.sh\"]\n",
    "\n",
    "\n",
    "# Use the official Python image from Docker Hub\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY ./src /app\n",
    "COPY requirements.txt /app\n",
    "\n",
    "# Install any needed dependencies specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Run the Python script when the container launches\n",
    "CMD [\"python\", \"loc_scraper.py\"]\n",
    "```\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "\n",
    "### Gcloud cli\n",
    "After this you will have to install gcloud cli and configure you're local environment. I will write up some scripts in a subsequent post to automate this process... but for the time being check out this [\"link\"](https://cloud.google.com/sdk/docs/install)\n",
    "\n",
    "### Create the image\n",
    "\n",
    "In the repo there is a a bash script called ```build.sh``` that will need to be updated to according to your gcp project.\n",
    "\n",
    "```bash\n",
    "gcloud builds submit --region=us-west2 --config cloudbuild.yaml\n",
    "```\n",
    "\n",
    "It calls ```cloudbuild.yaml``` which might need to be updated for you, but the following the should work.\n",
    "\n",
    "```yaml\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  script: |\n",
    "    docker build -t us-west2-docker.pkg.dev/$PROJECT_ID/supreme-court-scraper/supreme-court-scraper-image:dev .\n",
    "  automapSubstitutions: true\n",
    "images:\n",
    "- 'us-west2-docker.pkg.dev/$PROJECT_ID/supreme-court-scraper/supreme-court-scraper-image:dev'\n",
    "```\n",
    "\n",
    "### Following creation of the imge \n",
    "Next you can create a job on gcp by runnning the ```job_create.sh``` script... or by copying the code below and chaging yourproject to the correct project-name\n",
    "\n",
    "```bash\n",
    "gcloud run jobs create supreme-court-scraper --image us-west2-docker.pkg.dev/yourproject/supreme-court-scraper/supreme-court-scraper-image:dev \\\n",
    "```\n",
    "\n",
    "### Executing the job\n",
    "\n",
    "Once complete you can execute the job by running the ```execute_job.sh``` script or by running \n",
    "\n",
    "```bash\n",
    "gcloud run jobs execute supreme-court-scraper\n",
    "```\n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "In a perfect world the following should work. Note that src/.env should be set with your environmental variables such as ```$GCPPROJECTID``` \n",
    "\n",
    "```bash\n",
    "source src/.env \\\n",
    "&& ./build.sh \\ \n",
    "&& ./job_create.sh \\\n",
    "&& ./execute_job.sh\n",
    "```\n",
    "\n",
    "## Running locally\n",
    "\n",
    "The python script in the ```/src``` can be run locally, however it should be modified if you choose not to use gcp.  There are a number of functions within that can easily be modified to permit writing to the local directory. \n",
    "\n",
    "\n",
    "## Documentation Sources\n",
    "1. [\"Google Cloud Run Jobs Automation\"](https://cloud.google.com/run/docs/create-jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00aa040-909b-46f5-a750-56d651d78855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (4.12.3)\n",
      "Requirement already satisfied: bs4==0.0.2 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (0.0.2)\n",
      "Requirement already satisfied: cachetools==5.3.3 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (5.3.3)\n",
      "Requirement already satisfied: certifi==2024.2.2 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: flatten-json==0.1.14 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (0.1.14)\n",
      "Requirement already satisfied: google-api-core==2.18.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: google-auth==2.29.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (2.29.0)\n",
      "Requirement already satisfied: google-cloud-appengine-logging==1.4.3 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (1.4.3)\n",
      "Requirement already satisfied: google-cloud-audit-log==0.2.5 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 10)) (0.2.5)\n",
      "Requirement already satisfied: google-cloud-core==2.4.1 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 11)) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-logging==3.10.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: google-cloud-storage==2.16.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 13)) (2.16.0)\n",
      "Requirement already satisfied: google-crc32c==1.5.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 14)) (1.5.0)\n",
      "Requirement already satisfied: google-resumable-media==2.7.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 15)) (2.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos==1.63.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 16)) (1.63.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1==0.13.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 17)) (0.13.0)\n",
      "Requirement already satisfied: grpcio==1.62.2 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 18)) (1.62.2)\n",
      "Requirement already satisfied: grpcio-status==1.62.2 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 19)) (1.62.2)\n",
      "Requirement already satisfied: idna==3.7 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 20)) (3.7)\n",
      "Requirement already satisfied: proto-plus==1.23.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 21)) (1.23.0)\n",
      "Requirement already satisfied: protobuf==4.25.3 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 22)) (4.25.3)\n",
      "Requirement already satisfied: pyasn1==0.6.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 23)) (0.6.0)\n",
      "Requirement already satisfied: pyasn1_modules==0.4.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 24)) (0.4.0)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 25)) (2.31.0)\n",
      "Requirement already satisfied: rsa==4.9 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 26)) (4.9)\n",
      "Requirement already satisfied: six==1.16.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 27)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 28)) (2.5)\n",
      "Requirement already satisfied: urllib3==2.2.1 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 29)) (2.2.1)\n",
      "Requirement already satisfied: google-cloud-bigquery==3.25.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 40)) (3.25.0)\n",
      "Collecting numpy==2.0.0 (from -r ../requirements.txt (line 51))\n",
      "  Downloading numpy-2.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging==24.1 (from -r ../requirements.txt (line 52))\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas==2.2.2 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 53)) (2.2.2)\n",
      "Requirement already satisfied: pyarrow==16.1.0 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 56)) (16.1.0)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r ../requirements.txt (line 59))\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz==2024.1 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 60)) (2024.1)\n",
      "Requirement already satisfied: tzdata==2024.1 in /home/cobra/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from -r ../requirements.txt (line 65)) (2024.1)\n",
      "Downloading numpy-2.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-dateutil, packaging, numpy\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0\n",
      "    Uninstalling python-dateutil-2.9.0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.0.0 packaging-24.1 python-dateutil-2.9.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d88c9b-b4a7-411e-a6ee-68295bb906c4",
   "metadata": {},
   "source": [
    "## Write out the Script\n",
    "\n",
    "### Steps\n",
    "1. Initialize the Google Logging Service\n",
    "2. Initialize The Google Cloud Storage Service\n",
    "1. Initialize the Bigquery Client\n",
    "2. Grab a json blob\n",
    "3. Process the blob\n",
    "4. Move the blob to a processed bucket\n",
    "\n",
    "\n",
    "#### Initialize The Google Cloud Storage Service\n",
    "\n",
    "I created a Gloud Service Client Class available at : https://github.com/justin-napolitano/gcputils/blob/bc421debf4c828522580ec79ab634b2e2bf402a4/GoogleCloudLogging.py\n",
    "\n",
    "It is imported below and tested below.  Note that cli specific arguments are commented out for testing in ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4a000a-93b5-46c5-8a24-c9e272647c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_flattener.py\n",
    "# library_of_congress_scraper.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from gcputils.gcpclient import GCSClient\n",
    "from gcputils.GoogleCloudLogging import GoogleCloudLogging\n",
    "from gcputils.BigQueryClient import BigQueryClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "import html\n",
    "from flatten_json import flatten\n",
    "import google.cloud.logging\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f644ea-9dc6-4a7c-9de0-b9bfff9e4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab3a05-31ba-4a90-a3db-4a4faa58eb79",
   "metadata": {},
   "source": [
    "#### Initialize the Google Cloud Storage Client\n",
    "\n",
    "The Google Cloud Storage Client is available at https://github.com/justin-napolitano/gcputils/blob/bc421debf4c828522580ec79ab634b2e2bf402a4/gcpclient.py\n",
    "\n",
    "Calling the client and listing the buckets to test below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa26aa2-f203-4ad9-9c1d-c4647e5469c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e5e8c-3a35-4bee-a0d2-cac6b8cfbe3b",
   "metadata": {},
   "source": [
    "#### Access the Blobs within the bucket\n",
    "\n",
    "Now I need to grab a blob from the bucket. IN this case I just want to grab one from the top of the heap without pulling a lot of data into context. \n",
    "\n",
    "##### Addition to the storage class \n",
    "\n",
    "```Python\n",
    "\n",
    "def list_blobs(self, bucket_name):\n",
    "        \"\"\"\n",
    "        Lists all blobs in the specified bucket in Google Cloud Storage.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): Name of the bucket.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of blob names.\n",
    "        \"\"\"\n",
    "        # Get the bucket\n",
    "        bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "        # List all blobs in the bucket\n",
    "        blobs = list(bucket.list_blobs())\n",
    "        \n",
    "        blob_names = [blob.name for blob in blobs]\n",
    "        return blob_names\n",
    "\n",
    "def pop_blob(self, bucket_name):\n",
    "        \"\"\"\n",
    "        Selects and removes the first blob from the specified bucket in Google Cloud Storage.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): Name of the bucket.\n",
    "\n",
    "        Returns:\n",
    "            google.cloud.storage.blob.Blob: The first blob from the bucket.\n",
    "        \"\"\"\n",
    "        # Get the bucket\n",
    "        bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "        # List all blobs in the bucket\n",
    "        blobs = list(bucket.list_blobs())\n",
    "        \n",
    "        if not blobs:\n",
    "            print(f\"No blobs found in bucket '{bucket_name}'.\")\n",
    "            return None\n",
    "\n",
    "        # Get the first blob\n",
    "        first_blob = blobs[0]\n",
    "        \n",
    "        print(f\"First blob selected: {first_blob.name}\")\n",
    "        return first_blob\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc61fc-b792-4537-b01b-c8e8d20262d7",
   "metadata": {},
   "source": [
    "##### Test Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fb7e1d-8be9-4622-97c3-8a9ac93cbe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "First valid blob selected: last_page.txt\n",
      "First blob name: last_page.txt\n"
     ]
    }
   ],
   "source": [
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "\n",
    "    # Grab A blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name)\n",
    "    if first_blob:\n",
    "        print(f\"First blob name: {first_blob.name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ace3ce-f9d4-489d-b485-0551a3081539",
   "metadata": {},
   "source": [
    "#### Some additions to avoid last_page.txt\n",
    "\n",
    "So there is a last page.txt that is used by the scraper program. I want to pass some regex patterns to exclude in the pop_blob method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d350943e-a287-4e86-b662-e64e24c4ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_blob(self, bucket_name, patterns_file = None):\n",
    "        \"\"\"\n",
    "        Selects and removes the first blob from the specified bucket in Google Cloud Storage,\n",
    "        excluding any blobs that match patterns from the provided file.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): Name of the bucket.\n",
    "            patterns_file (str, optional): Path to the file containing regex patterns to exclude.\n",
    "\n",
    "        Returns:\n",
    "            google.cloud.storage.blob.Blob: The first blob from the bucket that doesn't match any pattern.\n",
    "        \"\"\"\n",
    "        # Load regex patterns from file\n",
    "        patterns = []\n",
    "        if patterns_file:\n",
    "            with open(patterns_file, 'r') as file:\n",
    "                patterns = [line.strip() for line in file]\n",
    "\n",
    "        # Get the bucket\n",
    "        bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "        # List all blobs in the bucket\n",
    "        blobs = list(bucket.list_blobs())\n",
    "        \n",
    "        if not blobs:\n",
    "            print(f\"No blobs found in bucket '{bucket_name}'.\")\n",
    "            return None\n",
    "\n",
    "        # Filter blobs based on regex patterns\n",
    "        for blob in blobs:\n",
    "            if not any(re.search(pattern, blob.name) for pattern in patterns):\n",
    "                print(f\"First valid blob selected: {blob.name}\")\n",
    "                return blob\n",
    "\n",
    "        print(\"No valid blobs found after applying regex patterns.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c65aafc-9424-43df-9ecf-a3898fa2aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "First valid blob selected: result-10.json\n",
      "First blob name: result-10.json\n"
     ]
    }
   ],
   "source": [
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "\n",
    "    # Grab A blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name,patterns_file )\n",
    "    if first_blob:\n",
    "        print(f\"First blob name: {first_blob.name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc252d-cc31-487d-bdd9-7c14285d87eb",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "Now I need to process the information. First off i need to grab the data from the blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa92274f-7ca7-4934-85c9-66c4339caf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob_to_memory(self, bucket_name, blob_name):\n",
    "        \"\"\"\n",
    "        Downloads a blob from the specified bucket to memory.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): Name of the bucket.\n",
    "            blob_name (str): Name of the blob to download.\n",
    "\n",
    "        Returns:\n",
    "            string: The string content of the blob.\n",
    "        \"\"\"\n",
    "        # Get the bucket\n",
    "        bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "        # Get the blob\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Download the blob to a string\n",
    "        blob_data = blob.download_as_string()\n",
    "        \n",
    "        # Parse the JSON content\n",
    "        # json_content = json.loads(blob_data)\n",
    "        \n",
    "        print(f\"Blob '{blob_name}' downloaded to memory.\")\n",
    "        return blob_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1466a6b5-b391-49cf-ad5d-a8f283b01d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "First valid blob selected: result-10.json\n",
      "First blob name: result-10.json\n",
      "Blob 'result-10.json' downloaded to memory.\n",
      "b'{\"breadcrumbs\": [{\"Library of Congress\": \"https://www.loc.gov\"}, {\"Digital Collections\": \"https://ww'\n"
     ]
    }
   ],
   "source": [
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "\n",
    "    # Grab A blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name,patterns_file )\n",
    "    if first_blob:\n",
    "        print(f\"First blob name: {first_blob.name}\")\n",
    "\n",
    "\n",
    "#download to memory\n",
    "\n",
    "    blob_data = gcs_client.download_blob_to_memory(bucket_name, first_blob.name)\n",
    "    json_data = json.loads(blob_data)\n",
    "    \n",
    "    print(blob_data[0:100])\n",
    "    # create_gcs_bucket(gcs_client, bucket_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f8fde-4a2e-404a-8848-46b3c14afc59",
   "metadata": {},
   "source": [
    "#### Flatten and process the JSON\n",
    "\n",
    "There is a ton of information in the json. I need to explore it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e46b8e0-e838-4f83-927a-b0de791f9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "First valid blob selected: result-10.json\n",
      "First blob name: result-10.json\n",
      "Blob 'result-10.json' downloaded to memory.\n",
      "b'{\"breadcrumbs\": [{\"Library of Congress\": \"https://www.loc.gov\"}, {\"Digital Collections\": \"https://ww'\n"
     ]
    }
   ],
   "source": [
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "credentials_path = None\n",
    "# if args.local:\n",
    "credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "# Initialize logging\n",
    "logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "logging_client.setup_logging()\n",
    "\n",
    "# List Buckets for testing\n",
    "gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "list_gcs_buckets(gcs_client)\n",
    "\n",
    "# Grab A blob from the heap\n",
    "first_blob = gcs_client.pop_blob(bucket_name,patterns_file )\n",
    "if first_blob:\n",
    "    print(f\"First blob name: {first_blob.name}\")\n",
    "\n",
    "\n",
    "#download to memory\n",
    "\n",
    "blob_data = gcs_client.download_blob_to_memory(bucket_name, first_blob.name)\n",
    "json_data = json.loads(blob_data)\n",
    "    \n",
    "print(blob_data[0:100])\n",
    "    # create_gcs_bucket(gcs_client, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071038ca-b2d4-482e-bd8f-e367c47b9b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['breadcrumbs', 'browse', 'categories', 'content', 'content_is_post', 'expert_resources', 'facet_trail', 'facet_views', 'facets', 'form_facets', 'next', 'next_sibling', 'options', 'original_formats', 'pages', 'pagination', 'partof', 'previous', 'previous_sibling', 'research-centers', 'results', 'search', 'shards', 'site_type', 'subjects', 'timestamp', 'title', 'topics', 'views'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cdbf626-a644-4bc0-b211-7135a913c06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_restricted': False,\n",
       " 'aka': ['http://www.loc.gov/item/usrep308213/',\n",
       "  'http://www.loc.gov/resource/usrep.usrep308213/',\n",
       "  'http://www.loc.gov/item/usrep.usrep308213/'],\n",
       " 'campaigns': [],\n",
       " 'contributor': ['stone, harlan fiske', 'supreme court of the united states'],\n",
       " 'date': '1939',\n",
       " 'dates': ['1939'],\n",
       " 'digitized': True,\n",
       " 'extract_timestamp': '2023-12-04T18:41:50.547Z',\n",
       " 'group': ['usrep103', 'us-report'],\n",
       " 'hassegments': False,\n",
       " 'id': 'http://www.loc.gov/item/usrep308213/',\n",
       " 'image_url': ['https://tile.loc.gov/storage-services/service/ll/usrep/usrep308/usrep308213/usrep308213.gif#h=150&w=100'],\n",
       " 'index': 631,\n",
       " 'item': {'call_number': ['Call Number: KF101',\n",
       "   'Series: Administrative Law',\n",
       "   'Series: Volume 308'],\n",
       "  'contributors': ['Stone, Harlan Fiske (Judge)',\n",
       "   'Supreme Court of the United States (Author)'],\n",
       "  'created_published': ['1939'],\n",
       "  'date': '19390000',\n",
       "  'format': 'periodical',\n",
       "  'genre': ['Periodical'],\n",
       "  'language': ['eng'],\n",
       "  'notes': ['Description: U.S. Reports Volume 308; October Term, 1939; Union Stock Yard & Transit Co. v. United States et al.'],\n",
       "  'rights': 'no known restrictions on use or reproduction',\n",
       "  'source_collection': 'U.S. Reports',\n",
       "  'subjects': ['Livestock',\n",
       "   'Law',\n",
       "   'Railroads',\n",
       "   'Law Library',\n",
       "   'Supreme Court',\n",
       "   'United States',\n",
       "   'Government Documents',\n",
       "   'Judicial review and appeals',\n",
       "   'Agency',\n",
       "   'Tariffs',\n",
       "   'Interstate commerce',\n",
       "   'Administrative law and regulatory procedure',\n",
       "   'U.S. Reports',\n",
       "   'Common law',\n",
       "   'Court opinions',\n",
       "   'Judicial decisions',\n",
       "   'Court cases',\n",
       "   'Court decisions',\n",
       "   'Interstate Commerce Commission (I.C.C.)',\n",
       "   'Agency jurisdiction',\n",
       "   'Periodical'],\n",
       "  'title': 'U.S. Reports: Union Stock Yard Co. v. U.S., 308 U.S. 213 (1939).'},\n",
       " 'language': ['english'],\n",
       " 'mime_type': ['image/gif', 'application/pdf'],\n",
       " 'online_format': ['image', 'pdf'],\n",
       " 'original_format': ['periodical'],\n",
       " 'other_title': [],\n",
       " 'partof': ['u.s. reports: volume 308',\n",
       "  'u.s. reports: administrative law',\n",
       "  'law library of congress',\n",
       "  'united states reports (official opinions of the u.s. supreme court)'],\n",
       " 'resources': [{'files': 1,\n",
       "   'image': 'https://tile.loc.gov/storage-services/service/ll/usrep/usrep308/usrep308213/usrep308213.gif',\n",
       "   'pdf': 'https://tile.loc.gov/storage-services/service/ll/usrep/usrep308/usrep308213/usrep308213.pdf',\n",
       "   'url': 'https://www.loc.gov/resource/usrep.usrep308213/'}],\n",
       " 'shelf_id': 'Call Number: KF101 Series: Administrative Law Series: Volume 308',\n",
       " 'subject': ['administrative law',\n",
       "  'livestock',\n",
       "  'railroads',\n",
       "  'supreme court',\n",
       "  'united states',\n",
       "  'court opinions',\n",
       "  'periodical',\n",
       "  'agency',\n",
       "  'interstate commerce',\n",
       "  'court cases',\n",
       "  'judicial decisions',\n",
       "  'law library',\n",
       "  'interstate commerce commission (i.c.c.)',\n",
       "  'judicial review and appeals',\n",
       "  'government documents',\n",
       "  'administrative law and regulatory procedure',\n",
       "  'law',\n",
       "  'common law',\n",
       "  'court decisions',\n",
       "  'u.s. reports',\n",
       "  'tariffs',\n",
       "  'agency jurisdiction'],\n",
       " 'subject_major_case_topic': ['administrative law'],\n",
       " 'timestamp': '2023-12-04T19:05:12.397Z',\n",
       " 'title': 'U.S. Reports: Union Stock Yard Co. v. U.S., 308 U.S. 213 (1939).',\n",
       " 'type': ['periodical'],\n",
       " 'url': 'https://www.loc.gov/item/usrep308213/'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[\"results\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a0292-adff-4242-a1c8-c207dcbf5bd4",
   "metadata": {},
   "source": [
    "#### Use Pandas to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0a20956-9633-4ee5-8f39-f720282211b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "       # Flatten the JSON content\n",
    "df_main = pd.json_normalize(json_data[\"results\"][0])\n",
    "        \n",
    "        # Normalize nested structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e1c12f-0b5b-46fa-9bba-e63294b93889",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = json_data[\"results\"][0]['item']\n",
    "resources_data = json_data[\"results\"][0]['resources']\n",
    "        \n",
    "df_item = pd.json_normalize(item_data)\n",
    "df_item['id'] = json_data[\"results\"][0]['id']  # Add 'id' for joining\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ec9461-08f8-442c-9439-3a07f3b013f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resources = pd.json_normalize(resources_data)\n",
    "df_resources['id'] = json_data[\"results\"][0]['id']  # Add 'id' for joining\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84292aae-a50c-440c-a902-eb92be2a35ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call_number</th>\n",
       "      <th>contributors</th>\n",
       "      <th>created_published</th>\n",
       "      <th>date</th>\n",
       "      <th>format</th>\n",
       "      <th>genre</th>\n",
       "      <th>language</th>\n",
       "      <th>notes</th>\n",
       "      <th>rights</th>\n",
       "      <th>source_collection</th>\n",
       "      <th>subjects</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Call Number: KF101, Series: Administrative La...</td>\n",
       "      <td>[Stone, Harlan Fiske (Judge), Supreme Court of...</td>\n",
       "      <td>[1939]</td>\n",
       "      <td>19390000</td>\n",
       "      <td>periodical</td>\n",
       "      <td>[Periodical]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Description: U.S. Reports Volume 308; October...</td>\n",
       "      <td>no known restrictions on use or reproduction</td>\n",
       "      <td>U.S. Reports</td>\n",
       "      <td>[Livestock, Law, Railroads, Law Library, Supre...</td>\n",
       "      <td>U.S. Reports: Union Stock Yard Co. v. U.S., 30...</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         call_number  \\\n",
       "0  [Call Number: KF101, Series: Administrative La...   \n",
       "\n",
       "                                        contributors created_published  \\\n",
       "0  [Stone, Harlan Fiske (Judge), Supreme Court of...            [1939]   \n",
       "\n",
       "       date      format         genre language  \\\n",
       "0  19390000  periodical  [Periodical]    [eng]   \n",
       "\n",
       "                                               notes  \\\n",
       "0  [Description: U.S. Reports Volume 308; October...   \n",
       "\n",
       "                                         rights source_collection  \\\n",
       "0  no known restrictions on use or reproduction      U.S. Reports   \n",
       "\n",
       "                                            subjects  \\\n",
       "0  [Livestock, Law, Railroads, Law Library, Supre...   \n",
       "\n",
       "                                               title  \\\n",
       "0  U.S. Reports: Union Stock Yard Co. v. U.S., 30...   \n",
       "\n",
       "                                     id  \n",
       "0  http://www.loc.gov/item/usrep308213/  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "394ee15f-396e-456d-b562-cf79e09d5522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>files</th>\n",
       "      <th>image</th>\n",
       "      <th>pdf</th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://tile.loc.gov/storage-services/service/...</td>\n",
       "      <td>https://tile.loc.gov/storage-services/service/...</td>\n",
       "      <td>https://www.loc.gov/resource/usrep.usrep308213/</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   files                                              image  \\\n",
       "0      1  https://tile.loc.gov/storage-services/service/...   \n",
       "\n",
       "                                                 pdf  \\\n",
       "0  https://tile.loc.gov/storage-services/service/...   \n",
       "\n",
       "                                               url  \\\n",
       "0  https://www.loc.gov/resource/usrep.usrep308213/   \n",
       "\n",
       "                                     id  \n",
       "0  http://www.loc.gov/item/usrep308213/  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73690bc0-4be8-4709-92e1-6482142321e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access_restricted</th>\n",
       "      <th>aka</th>\n",
       "      <th>campaigns</th>\n",
       "      <th>contributor</th>\n",
       "      <th>date</th>\n",
       "      <th>dates</th>\n",
       "      <th>digitized</th>\n",
       "      <th>extract_timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>hassegments</th>\n",
       "      <th>...</th>\n",
       "      <th>item.created_published</th>\n",
       "      <th>item.date</th>\n",
       "      <th>item.format</th>\n",
       "      <th>item.genre</th>\n",
       "      <th>item.language</th>\n",
       "      <th>item.notes</th>\n",
       "      <th>item.rights</th>\n",
       "      <th>item.source_collection</th>\n",
       "      <th>item.subjects</th>\n",
       "      <th>item.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>[http://www.loc.gov/item/usrep308213/, http://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[stone, harlan fiske, supreme court of the uni...</td>\n",
       "      <td>1939</td>\n",
       "      <td>[1939]</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-12-04T18:41:50.547Z</td>\n",
       "      <td>[usrep103, us-report]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[1939]</td>\n",
       "      <td>19390000</td>\n",
       "      <td>periodical</td>\n",
       "      <td>[Periodical]</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Description: U.S. Reports Volume 308; October...</td>\n",
       "      <td>no known restrictions on use or reproduction</td>\n",
       "      <td>U.S. Reports</td>\n",
       "      <td>[Livestock, Law, Railroads, Law Library, Supre...</td>\n",
       "      <td>U.S. Reports: Union Stock Yard Co. v. U.S., 30...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   access_restricted                                                aka  \\\n",
       "0              False  [http://www.loc.gov/item/usrep308213/, http://...   \n",
       "\n",
       "  campaigns                                        contributor  date   dates  \\\n",
       "0        []  [stone, harlan fiske, supreme court of the uni...  1939  [1939]   \n",
       "\n",
       "   digitized         extract_timestamp                  group  hassegments  \\\n",
       "0       True  2023-12-04T18:41:50.547Z  [usrep103, us-report]        False   \n",
       "\n",
       "   ... item.created_published item.date  item.format    item.genre  \\\n",
       "0  ...                 [1939]  19390000   periodical  [Periodical]   \n",
       "\n",
       "  item.language                                         item.notes  \\\n",
       "0         [eng]  [Description: U.S. Reports Volume 308; October...   \n",
       "\n",
       "                                    item.rights item.source_collection  \\\n",
       "0  no known restrictions on use or reproduction           U.S. Reports   \n",
       "\n",
       "                                       item.subjects  \\\n",
       "0  [Livestock, Law, Railroads, Law Library, Supre...   \n",
       "\n",
       "                                          item.title  \n",
       "0  U.S. Reports: Union Stock Yard Co. v. U.S., 30...  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b8e09cb-d611-46d9-ad22-87c2b8c1d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_call_number = pd.DataFrame(df_item[\"call_number\"], columns=['call_number'])\n",
    "# df_call_number['id'] = json_data[\"results\"][0]['id']  # Add 'id' for joining\n",
    "\n",
    "call_numbers = item_data.get('call_number', [])\n",
    "df_call_number = pd.DataFrame(call_numbers, columns=['call_number'])\n",
    "df_call_number['id'] = json_data[\"results\"][0]['id']  # Add 'id' for joining\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e24414-0012-42ea-ad5a-1921aaf6a8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call_number</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Call Number: KF101</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Series: Administrative Law</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Series: Volume 308</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  call_number                                    id\n",
       "0          Call Number: KF101  http://www.loc.gov/item/usrep308213/\n",
       "1  Series: Administrative Law  http://www.loc.gov/item/usrep308213/\n",
       "2          Series: Volume 308  http://www.loc.gov/item/usrep308213/"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_call_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43e8f4cd-0ea2-4678-95d2-ae85838a0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = item_data.get('subjects', [])\n",
    "df_subjects = pd.DataFrame(subjects, columns=['subjects'])\n",
    "df_subjects['id'] = json_data[\"results\"][0]['id']  # Add 'id' for joining\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62971499-4c24-45ba-ad87-5b27e2d8a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjects</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Livestock</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Law</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Railroads</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law Library</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supreme Court</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>United States</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Government Documents</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Judicial review and appeals</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Agency</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tariffs</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Interstate commerce</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Administrative law and regulatory procedure</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>U.S. Reports</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Common law</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Court opinions</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Judicial decisions</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Court cases</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Court decisions</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Interstate Commerce Commission (I.C.C.)</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Agency jurisdiction</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Periodical</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       subjects  \\\n",
       "0                                     Livestock   \n",
       "1                                           Law   \n",
       "2                                     Railroads   \n",
       "3                                   Law Library   \n",
       "4                                 Supreme Court   \n",
       "5                                 United States   \n",
       "6                          Government Documents   \n",
       "7                   Judicial review and appeals   \n",
       "8                                        Agency   \n",
       "9                                       Tariffs   \n",
       "10                          Interstate commerce   \n",
       "11  Administrative law and regulatory procedure   \n",
       "12                                 U.S. Reports   \n",
       "13                                   Common law   \n",
       "14                               Court opinions   \n",
       "15                           Judicial decisions   \n",
       "16                                  Court cases   \n",
       "17                              Court decisions   \n",
       "18      Interstate Commerce Commission (I.C.C.)   \n",
       "19                          Agency jurisdiction   \n",
       "20                                   Periodical   \n",
       "\n",
       "                                      id  \n",
       "0   http://www.loc.gov/item/usrep308213/  \n",
       "1   http://www.loc.gov/item/usrep308213/  \n",
       "2   http://www.loc.gov/item/usrep308213/  \n",
       "3   http://www.loc.gov/item/usrep308213/  \n",
       "4   http://www.loc.gov/item/usrep308213/  \n",
       "5   http://www.loc.gov/item/usrep308213/  \n",
       "6   http://www.loc.gov/item/usrep308213/  \n",
       "7   http://www.loc.gov/item/usrep308213/  \n",
       "8   http://www.loc.gov/item/usrep308213/  \n",
       "9   http://www.loc.gov/item/usrep308213/  \n",
       "10  http://www.loc.gov/item/usrep308213/  \n",
       "11  http://www.loc.gov/item/usrep308213/  \n",
       "12  http://www.loc.gov/item/usrep308213/  \n",
       "13  http://www.loc.gov/item/usrep308213/  \n",
       "14  http://www.loc.gov/item/usrep308213/  \n",
       "15  http://www.loc.gov/item/usrep308213/  \n",
       "16  http://www.loc.gov/item/usrep308213/  \n",
       "17  http://www.loc.gov/item/usrep308213/  \n",
       "18  http://www.loc.gov/item/usrep308213/  \n",
       "19  http://www.loc.gov/item/usrep308213/  \n",
       "20  http://www.loc.gov/item/usrep308213/  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f413869-6950-4af6-93c3-88e6e899582b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Description: U.S. Reports Volume 308; October ...</td>\n",
       "      <td>http://www.loc.gov/item/usrep308213/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               notes  \\\n",
       "0  Description: U.S. Reports Volume 308; October ...   \n",
       "\n",
       "                                     id  \n",
       "0  http://www.loc.gov/item/usrep308213/  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = item_data.get('notes', [])\n",
    "df_notes = pd.DataFrame(notes, columns=['notes'])\n",
    "df_notes['id'] = json_data[\"results\"][0]['id']  # Add 'id' for joining\n",
    "df_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7d222-ff33-492a-9b13-6f5cf7abd100",
   "metadata": {},
   "source": [
    "##### Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36cb35af-8eb0-4aea-b097-abec442d209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_main(result):\n",
    "    df_main = pd.json_normalize(result)\n",
    "    return df_main\n",
    "\n",
    "def normalize_item(result):\n",
    "    item_data = result['item']\n",
    "    df_item = pd.json_normalize(item_data)\n",
    "    df_item['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_item\n",
    "\n",
    "def normalize_resources(result):\n",
    "    resources_data = result['resources']\n",
    "    df_resources = pd.json_normalize(resources_data)\n",
    "    df_resources['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_resources\n",
    "\n",
    "def normalize_call_numbers(result):\n",
    "    item_data = result['item']\n",
    "    call_numbers = item_data.get('call_number', [])\n",
    "    df_call_number = pd.DataFrame(call_numbers, columns=['call_number'])\n",
    "    df_call_number['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_call_number\n",
    "\n",
    "def normalize_contributors(result):\n",
    "    item_data = result['item']\n",
    "    contributors = item_data.get('contributors', [])\n",
    "    df_contributors = pd.DataFrame(contributors, columns=['contributors'])\n",
    "    df_contributors['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_contributors\n",
    "\n",
    "def normalize_subjects(result):\n",
    "    item_data = result['item']\n",
    "    subjects = item_data.get('subjects', [])\n",
    "    df_subjects = pd.DataFrame(subjects, columns=['subjects'])\n",
    "    df_subjects['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_subjects\n",
    "\n",
    "def normalize_notes(result):\n",
    "    item_data = result['item']\n",
    "    notes = item_data.get('notes', [])\n",
    "    df_notes = pd.DataFrame(notes, columns=['notes'])\n",
    "    df_notes['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "892bcbce-b17a-4ca4-9d18-1894e7fb2ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "First valid blob selected: result-10.json\n",
      "First blob name: result-10.json\n",
      "Blob 'result-10.json' downloaded to memory.\n",
      "b'{\"breadcrumbs\": [{\"Library of Congress\": \"https://www.loc.gov\"}, {\"Digital Collections\": \"https://ww'\n"
     ]
    }
   ],
   "source": [
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_bq_client(project_id,credentials_path=None):\n",
    "    return BigQueryClient(project_id,credentials_path = credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "\n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "\n",
    "    # initialize bq\n",
    "\n",
    "    bq_client = initialize_bq_client(project_id,credentials_path)\n",
    "\n",
    "    # Grab A blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name,patterns_file )\n",
    "    if first_blob:\n",
    "        print(f\"First blob name: {first_blob.name}\")\n",
    "\n",
    "\n",
    "#download to memory\n",
    "\n",
    "    blob_data = gcs_client.download_blob_to_memory(bucket_name, first_blob.name)\n",
    "    json_data = json.loads(blob_data)\n",
    "    \n",
    "    print(blob_data[0:100])\n",
    "    # create_gcs_bucket(gcs_client, bucket_name)\n",
    "\n",
    "    results = json_data[\"results\"]\n",
    "\n",
    "    # Initialize lists to hold DataFramesdf_notes\n",
    "    df_main_list = []\n",
    "    df_item_list = []\n",
    "    df_resources_list = []\n",
    "    df_call_number_list = []\n",
    "    df_contributors_list = []\n",
    "    df_subjects_list = []\n",
    "    df_notes_list = []\n",
    "\n",
    "    for result in results:\n",
    "        df_main_list.append(normalize_main(result))\n",
    "        df_item_list.append(normalize_item(result))\n",
    "        df_resources_list.append(normalize_resources(result))\n",
    "        df_call_number_list.append(normalize_call_numbers(result))\n",
    "        df_contributors_list.append(normalize_contributors(result))\n",
    "        df_subjects_list.append(normalize_subjects(result))\n",
    "        df_notes_list.append(normalize_notes(result))\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    df_main = pd.concat(df_main_list, ignore_index=True)\n",
    "    df_item = pd.concat(df_item_list, ignore_index=True)\n",
    "    df_resources = pd.concat(df_resources_list, ignore_index=True)\n",
    "    df_call_number = pd.concat(df_call_number_list, ignore_index=True)\n",
    "    df_contributors = pd.concat(df_contributors_list, ignore_index=True)\n",
    "    df_subjects = pd.concat(df_subjects_list, ignore_index=True)\n",
    "    df_notes = pd.concat(df_notes_list, ignore_index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5eb8b57-ec39-4e18-ad7f-bf1823e4885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "Dataset supreme_court created.\n",
      "First valid blob selected: result-10.json\n",
      "First blob name: result-10.json\n",
      "Blob 'result-10.json' downloaded to memory.\n",
      "Table items_staging created in dataset supreme_court.\n",
      "Table resources_staging created in dataset supreme_court.\n",
      "Table call_numbers_staging created in dataset supreme_court.\n",
      "Table contributors_staging created in dataset supreme_court.\n",
      "Table subjects_staging created in dataset supreme_court.\n",
      "Table notes_staging created in dataset supreme_court.\n",
      "Loaded 70 rows into supreme_court:items_staging.\n",
      "Loaded 70 rows into supreme_court:resources_staging.\n",
      "Loaded 210 rows into supreme_court:call_numbers_staging.\n",
      "Loaded 138 rows into supreme_court:contributors_staging.\n",
      "Loaded 1570 rows into supreme_court:subjects_staging.\n",
      "Loaded 70 rows into supreme_court:notes_staging.\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "# parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_bq_client(project_id,credentials_path=None):\n",
    "    return BigQueryClient(project_id,credentials_path = credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    main_table_id = \"results_staging\"\n",
    "    item_table_id = \"items_staging\"\n",
    "    resources_table_id = \"resources_staging\"\n",
    "    call_number_table_id = \"call_numbers_staging\"\n",
    "    contributors_table_id = \"contributors_staging\"\n",
    "    subjects_table_id = \"subjects_staging\"\n",
    "    notes_table_id = \"notes_staging\"\n",
    "    \n",
    "    dataset_id = \"supreme_court\"\n",
    "    \n",
    "    patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "    \n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "    \n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "    \n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "    \n",
    "    bq_client = initialize_bq_client(project_id,credentials_path)\n",
    "    \n",
    "    # Create the dataset if not exists\n",
    "    \n",
    "    bq_client.create_dataset(dataset_id)\n",
    "    \n",
    "    # Grab A blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name,patterns_file )\n",
    "    if first_blob:\n",
    "        print(f\"First blob name: {first_blob.name}\")\n",
    "    \n",
    "    \n",
    "    #download to memory\n",
    "    \n",
    "    blob_data = gcs_client.download_blob_to_memory(bucket_name, first_blob.name)\n",
    "    json_data = json.loads(blob_data)\n",
    "    \n",
    "    # print(blob_data[0:100])\n",
    "    # create_gcs_bucket(gcs_client, bucket_name)\n",
    "    \n",
    "    results = json_data[\"results\"]\n",
    "    \n",
    "    # Initialize lists to hold DataFrames\n",
    "    df_main_list = []\n",
    "    df_item_list = []\n",
    "    df_resources_list = []\n",
    "    df_call_number_list = []\n",
    "    df_contributors_list = []\n",
    "    df_subjects_list = []\n",
    "    df_notes_list = []\n",
    "    \n",
    "    for result in results:\n",
    "        df_main_list.append(normalize_main(result))\n",
    "        df_item_list.append(normalize_item(result))\n",
    "        df_resources_list.append(normalize_resources(result))\n",
    "        df_call_number_list.append(normalize_call_numbers(result))\n",
    "        df_contributors_list.append(normalize_contributors(result))\n",
    "        df_subjects_list.append(normalize_subjects(result))\n",
    "        df_notes_list.append(normalize_notes(result))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    df_main = pd.concat(df_main_list, ignore_index=True)\n",
    "    df_item = pd.concat(df_item_list, ignore_index=True)\n",
    "    df_resources = pd.concat(df_resources_list, ignore_index=True)\n",
    "    df_call_number = pd.concat(df_call_number_list, ignore_index=True)\n",
    "    df_contributors = pd.concat(df_contributors_list, ignore_index=True)\n",
    "    df_subjects = pd.concat(df_subjects_list, ignore_index=True)\n",
    "    df_notes = pd.concat(df_notes_list, ignore_index=True)\n",
    "    \n",
    "    # Define the BigQuery table schema\n",
    "    # main_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_main.columns]\n",
    "    item_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_item.columns]\n",
    "    resources_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_resources.columns]\n",
    "    call_number_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_call_number.columns]\n",
    "    contributors_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_contributors.columns]\n",
    "    subjects_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_subjects.columns]\n",
    "    notes_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_notes.columns]\n",
    "    \n",
    "    # Create BigQuery tables\n",
    "    # bq_client.create_table(dataset_id, main_table_id, main_schema)\n",
    "    bq_client.create_table(dataset_id, item_table_id, item_schema)\n",
    "    bq_client.create_table(dataset_id, resources_table_id, resources_schema)\n",
    "    bq_client.create_table(dataset_id, call_number_table_id, call_number_schema)\n",
    "    bq_client.create_table(dataset_id, contributors_table_id, contributors_schema)\n",
    "    bq_client.create_table(dataset_id, subjects_table_id, subjects_schema)\n",
    "    bq_client.create_table(dataset_id, notes_table_id, notes_schema)\n",
    "    \n",
    "    # Load DataFrames into BigQuery tables\n",
    "    # bq_client.load_dataframe_to_table(dataset_id, main_table_id, df_main)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, item_table_id, df_item)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, resources_table_id, df_resources)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, call_number_table_id, df_call_number)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, contributors_table_id, df_contributors)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, subjects_table_id, df_subjects)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, notes_table_id, df_notes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e75c2-131d-4ace-a4d9-d657ea2e7e34",
   "metadata": {},
   "source": [
    "#### Moving the Processed blobs to a Processed Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf56b1-a12f-462a-9d71-ac35dd6297af",
   "metadata": {},
   "source": [
    "##### Add code to the GCS Client to enable deleting and copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "694ded4a-7c04-4e3e-ad1a-0400f323582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_blob(self, source_bucket_name, source_blob_name, destination_bucket_name, destination_blob_name):\n",
    "        \"\"\"\n",
    "        Copies a blob from one bucket to another.\n",
    "\n",
    "        Args:\n",
    "            source_bucket_name (str): Name of the source bucket.\n",
    "            source_blob_name (str): Name of the source blob.\n",
    "            destination_bucket_name (str): Name of the destination bucket.\n",
    "            destination_blob_name (str): Name of the destination blob.\n",
    "\n",
    "        Returns:\n",
    "            google.cloud.storage.blob.Blob: The copied blob.\n",
    "        \"\"\"\n",
    "        source_bucket = self.client.bucket(source_bucket_name)\n",
    "        source_blob = source_bucket.blob(source_blob_name)\n",
    "        destination_bucket = self.client.bucket(destination_bucket_name)\n",
    "        blob_copy = source_bucket.copy_blob(source_blob, destination_bucket, destination_blob_name)\n",
    "        return blob_copy\n",
    "\n",
    "def delete_blob(self, bucket_name, blob_name):\n",
    "        \"\"\"\n",
    "        Deletes a blob from the specified bucket.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): Name of the bucket.\n",
    "            blob_name (str): Name of the blob to delete.\n",
    "        \"\"\"\n",
    "        bucket = self.client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25fbaa-dd13-4b04-bba8-a9871baef052",
   "metadata": {},
   "source": [
    "##### Add a couple lines to the main script to call the new methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da228b1b-4e16-4f26-ad9b-269a5b234943",
   "metadata": {},
   "source": [
    "\n",
    "```Python\n",
    "# Move the blob to the processed_results bucket\n",
    "gcs_client.copy_blob(bucket_name, first_blob.name, processed_bucket_name, first_blob.name)\n",
    "gcs_client.delete_blob(bucket_name, first_blob.name)\n",
    "print(f\"Blob {first_blob.name} moved to {processed_bucket_name} and deleted from {bucket_name}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac12ae76-4ce9-4a71-bf0e-0a65c25c61a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying creds file\n",
      "Buckets: ['loc-scraper', 'loc_flattener_processed', 'processed_results', 'smart-axis-421517_cloudbuild']\n",
      "Dataset supreme_court created.\n",
      "First valid blob selected: result-10.json\n",
      "First blob name: result-10.json\n",
      "Blob 'result-10.json' downloaded to memory.\n",
      "Table items_staging created in dataset supreme_court.\n",
      "Table resources_staging created in dataset supreme_court.\n",
      "Table call_numbers_staging created in dataset supreme_court.\n",
      "Table contributors_staging created in dataset supreme_court.\n",
      "Table subjects_staging created in dataset supreme_court.\n",
      "Table notes_staging created in dataset supreme_court.\n",
      "Loaded 70 rows into supreme_court:items_staging.\n",
      "Loaded 70 rows into supreme_court:resources_staging.\n",
      "Loaded 210 rows into supreme_court:call_numbers_staging.\n",
      "Loaded 138 rows into supreme_court:contributors_staging.\n",
      "Loaded 1570 rows into supreme_court:subjects_staging.\n",
      "Loaded 70 rows into supreme_court:notes_staging.\n",
      "Blob result-10.json moved to loc_flattener_processed and deleted from loc-scraper\n"
     ]
    }
   ],
   "source": [
    "#### Testing\n",
    "# loc_flattener.py\n",
    "# library_of_congress_scraper.py\n",
    "\n",
    "# loc_flattener.py\n",
    "# library_of_congress_scraper.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from gcputils.gcpclient import GCSClient\n",
    "from gcputils.GoogleCloudLogging import GoogleCloudLogging\n",
    "from gcputils.BigQueryClient import BigQueryClient\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "import html\n",
    "from flatten_json import flatten\n",
    "import google.cloud.logging\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "    \n",
    "\n",
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_bq_client(project_id,credentials_path=None):\n",
    "    return BigQueryClient(project_id,credentials_path = credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def create_gcs_bucket(client, bucket_name):\n",
    "    try:\n",
    "        bucket = client.create_bucket(bucket_name=bucket_name)\n",
    "        logging.info(bucket)\n",
    "        print(bucket)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating bucket: {e}\")\n",
    "\n",
    "def normalize_main(result):\n",
    "    df_main = pd.json_normalize(result)\n",
    "    return df_main\n",
    "\n",
    "def normalize_item(result):\n",
    "    item_data = result['item']\n",
    "    df_item = pd.json_normalize(item_data)\n",
    "    df_item['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_item\n",
    "\n",
    "def normalize_resources(result):\n",
    "    resources_data = result['resources']\n",
    "    df_resources = pd.json_normalize(resources_data)\n",
    "    df_resources['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_resources\n",
    "\n",
    "def normalize_call_numbers(result):\n",
    "    item_data = result['item']\n",
    "    call_numbers = item_data.get('call_number', [])\n",
    "    df_call_number = pd.DataFrame(call_numbers, columns=['call_number'])\n",
    "    df_call_number['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_call_number\n",
    "\n",
    "def normalize_contributors(result):\n",
    "    item_data = result['item']\n",
    "    contributors = item_data.get('contributors', [])\n",
    "    df_contributors = pd.DataFrame(contributors, columns=['contributors'])\n",
    "    df_contributors['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_contributors\n",
    "\n",
    "def normalize_subjects(result):\n",
    "    item_data = result['item']\n",
    "    subjects = item_data.get('subjects', [])\n",
    "    df_subjects = pd.DataFrame(subjects, columns=['subjects'])\n",
    "    df_subjects['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_subjects\n",
    "\n",
    "def normalize_notes(result):\n",
    "    item_data = result['item']\n",
    "    notes = item_data.get('notes', [])\n",
    "    df_notes = pd.DataFrame(notes, columns=['notes'])\n",
    "    df_notes['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_notes\n",
    "\n",
    "def main():\n",
    "    # parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    # parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    main_table_id = \"results_staging\"\n",
    "    item_table_id = \"items_staging\"\n",
    "    resources_table_id = \"resources_staging\"\n",
    "    call_number_table_id = \"call_numbers_staging\"\n",
    "    contributors_table_id = \"contributors_staging\"\n",
    "    subjects_table_id = \"subjects_staging\"\n",
    "    notes_table_id = \"notes_staging\"\n",
    "\n",
    "    processed_bucket_name = \"loc_flattener_processed\"\n",
    "    \n",
    "    dataset_id = \"supreme_court\"\n",
    "    \n",
    "    patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "    \n",
    "    credentials_path = None\n",
    "    # if args.local:\n",
    "    credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "    \n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id,credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "    \n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "    \n",
    "    bq_client = initialize_bq_client(project_id,credentials_path)\n",
    "    \n",
    "    # Create the dataset if not exists\n",
    "    \n",
    "    bq_client.create_dataset(dataset_id)\n",
    "\n",
    "    # create the processed_bucket if not exists\n",
    "\n",
    "    # print(gcs_client.create_bucket(processed_bucket_name))\n",
    "    \n",
    "    # Grab A blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name,patterns_file )\n",
    "    if first_blob:\n",
    "        print(f\"First blob name: {first_blob.name}\")\n",
    "    \n",
    "    \n",
    "    #download to memory\n",
    "    \n",
    "    blob_data = gcs_client.download_blob_to_memory(bucket_name, first_blob.name)\n",
    "    json_data = json.loads(blob_data)\n",
    "    \n",
    "    # print(blob_data[0:100])\n",
    "    # create_gcs_bucket(gcs_client, bucket_name)\n",
    "    \n",
    "    results = json_data[\"results\"]\n",
    "    \n",
    "    # Initialize lists to hold DataFrames\n",
    "    df_main_list = []\n",
    "    df_item_list = []\n",
    "    df_resources_list = []\n",
    "    df_call_number_list = []\n",
    "    df_contributors_list = []\n",
    "    df_subjects_list = []\n",
    "    df_notes_list = []\n",
    "    \n",
    "    for result in results:\n",
    "        df_main_list.append(normalize_main(result))\n",
    "        df_item_list.append(normalize_item(result))\n",
    "        df_resources_list.append(normalize_resources(result))\n",
    "        df_call_number_list.append(normalize_call_numbers(result))\n",
    "        df_contributors_list.append(normalize_contributors(result))\n",
    "        df_subjects_list.append(normalize_subjects(result))\n",
    "        df_notes_list.append(normalize_notes(result))\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    df_main = pd.concat(df_main_list, ignore_index=True)\n",
    "    df_item = pd.concat(df_item_list, ignore_index=True)\n",
    "    df_resources = pd.concat(df_resources_list, ignore_index=True)\n",
    "    df_call_number = pd.concat(df_call_number_list, ignore_index=True)\n",
    "    df_contributors = pd.concat(df_contributors_list, ignore_index=True)\n",
    "    df_subjects = pd.concat(df_subjects_list, ignore_index=True)\n",
    "    df_notes = pd.concat(df_notes_list, ignore_index=True)\n",
    "    \n",
    "    # Define the BigQuery table schema\n",
    "    # main_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_main.columns]\n",
    "    item_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_item.columns]\n",
    "    resources_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_resources.columns]\n",
    "    call_number_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_call_number.columns]\n",
    "    contributors_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_contributors.columns]\n",
    "    subjects_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_subjects.columns]\n",
    "    notes_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_notes.columns]\n",
    "    \n",
    "    # Create BigQuery tables\n",
    "    # bq_client.create_table(dataset_id, main_table_id, main_schema)\n",
    "    bq_client.create_table(dataset_id, item_table_id, item_schema)\n",
    "    bq_client.create_table(dataset_id, resources_table_id, resources_schema)\n",
    "    bq_client.create_table(dataset_id, call_number_table_id, call_number_schema)\n",
    "    bq_client.create_table(dataset_id, contributors_table_id, contributors_schema)\n",
    "    bq_client.create_table(dataset_id, subjects_table_id, subjects_schema)\n",
    "    bq_client.create_table(dataset_id, notes_table_id, notes_schema)\n",
    "    \n",
    "    # Load DataFrames into BigQuery tables\n",
    "    # bq_client.load_dataframe_to_table(dataset_id, main_table_id, df_main)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, item_table_id, df_item)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, resources_table_id, df_resources)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, call_number_table_id, df_call_number)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, contributors_table_id, df_contributors)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, subjects_table_id, df_subjects)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, notes_table_id, df_notes)\n",
    "\n",
    "    # Move the blob to the processed_results bucket\n",
    "    gcs_client.copy_blob(bucket_name, first_blob.name, processed_bucket_name, first_blob.name)\n",
    "    # gcs_client.delete_blob(bucket_name, first_blob.name)\n",
    "    print(f\"Blob {first_blob.name} moved to {processed_bucket_name} and deleted from {bucket_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb59df-fb38-472b-b612-c0a6d0a9abcf",
   "metadata": {},
   "source": [
    "#### Add while true logic and clean up the script\n",
    "\n",
    "```Python\n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "from gcputils.gcpclient import GCSClient\n",
    "from gcputils.GoogleCloudLogging import GoogleCloudLogging\n",
    "from gcputils.BigQueryClient import BigQueryClient\n",
    "\n",
    "def initialize_gcs_client(project_id, credentials_path=None):\n",
    "    return GCSClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_google_cloud_logging_client(project_id, credentials_path=None):\n",
    "    return GoogleCloudLogging(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def initialize_bq_client(project_id, credentials_path=None):\n",
    "    return BigQueryClient(project_id, credentials_path=credentials_path)\n",
    "\n",
    "def list_gcs_buckets(client):\n",
    "    try:\n",
    "        buckets = client.list_buckets()\n",
    "        print(\"Buckets:\", buckets)\n",
    "        logging.info(f\"Buckets: {buckets}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing buckets: {e}\")\n",
    "\n",
    "def create_gcs_bucket(client, bucket_name):\n",
    "    try:\n",
    "        bucket = client.create_bucket(bucket_name=bucket_name)\n",
    "        logging.info(bucket)\n",
    "        print(bucket)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating bucket: {e}\")\n",
    "\n",
    "def normalize_main(result):\n",
    "    df_main = pd.json_normalize(result)\n",
    "    return df_main\n",
    "\n",
    "def normalize_item(result):\n",
    "    item_data = result['item']\n",
    "    df_item = pd.json_normalize(item_data)\n",
    "    df_item['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_item\n",
    "\n",
    "def normalize_resources(result):\n",
    "    resources_data = result['resources']\n",
    "    df_resources = pd.json_normalize(resources_data)\n",
    "    df_resources['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_resources\n",
    "\n",
    "def normalize_call_numbers(result):\n",
    "    item_data = result['item']\n",
    "    call_numbers = item_data.get('call_number', [])\n",
    "    df_call_number = pd.DataFrame(call_numbers, columns=['call_number'])\n",
    "    df_call_number['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_call_number\n",
    "\n",
    "def normalize_contributors(result):\n",
    "    item_data = result['item']\n",
    "    contributors = item_data.get('contributors', [])\n",
    "    df_contributors = pd.DataFrame(contributors, columns=['contributors'])\n",
    "    df_contributors['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_contributors\n",
    "\n",
    "def normalize_subjects(result):\n",
    "    item_data = result['item']\n",
    "    subjects = item_data.get('subjects', [])\n",
    "    df_subjects = pd.DataFrame(subjects, columns=['subjects'])\n",
    "    df_subjects['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_subjects\n",
    "\n",
    "def normalize_notes(result):\n",
    "    item_data = result['item']\n",
    "    notes = item_data.get('notes', [])\n",
    "    df_notes = pd.DataFrame(notes, columns=['notes'])\n",
    "    df_notes['id'] = result['id']  # Add 'id' for joining\n",
    "    return df_notes\n",
    "\n",
    "def create_tables_and_schemas(bq_client, bucket_name, patterns_file, gcs_client, dataset_id):\n",
    "    # Define the BigQuery table schema\n",
    "    main_table_id = \"results_staging\"\n",
    "    item_table_id = \"items_staging\"\n",
    "    resources_table_id = \"resources_staging\"\n",
    "    call_number_table_id = \"call_numbers_staging\"\n",
    "    contributors_table_id = \"contributors_staging\"\n",
    "    subjects_table_id = \"subjects_staging\"\n",
    "    notes_table_id = \"notes_staging\"\n",
    "\n",
    "    # Assuming the first blob provides a sample structure\n",
    "    sample_blob = gcs_client.pop_blob(bucket_name, patterns_file)\n",
    "    blob_data = gcs_client.download_blob_to_memory(bucket_name, sample_blob.name)\n",
    "    json_data = json.loads(blob_data)\n",
    "    result = json_data[\"results\"][0]  # Use the first result as a sample\n",
    "\n",
    "    df_main = normalize_main(result)\n",
    "    df_item = normalize_item(result)\n",
    "    df_resources = normalize_resources(result)\n",
    "    df_call_number = normalize_call_numbers(result)\n",
    "    df_contributors = normalize_contributors(result)\n",
    "    df_subjects = normalize_subjects(result)\n",
    "    df_notes = normalize_notes(result)\n",
    "\n",
    "    main_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_main.columns]\n",
    "    item_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_item.columns]\n",
    "    resources_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_resources.columns]\n",
    "    call_number_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_call_number.columns]\n",
    "    contributors_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_contributors.columns]\n",
    "    subjects_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_subjects.columns]\n",
    "    notes_schema = [bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING) for name in df_notes.columns]\n",
    "\n",
    "    # Create BigQuery tables\n",
    "    # bq_client.create_table(dataset_id, main_table_id, main_schema)\n",
    "    bq_client.create_table(dataset_id, item_table_id, item_schema)\n",
    "    bq_client.create_table(dataset_id, resources_table_id, resources_schema)\n",
    "    bq_client.create_table(dataset_id, call_number_table_id, call_number_schema)\n",
    "    bq_client.create_table(dataset_id, contributors_table_id, contributors_schema)\n",
    "    bq_client.create_table(dataset_id, subjects_table_id, subjects_schema)\n",
    "    bq_client.create_table(dataset_id, notes_table_id, notes_schema)\n",
    "\n",
    "def process_blob(gcs_client, bq_client, bucket_name, processed_bucket_name, patterns_file, dataset_id):\n",
    "    main_table_id = \"results_staging\"\n",
    "    item_table_id = \"items_staging\"\n",
    "    resources_table_id = \"resources_staging\"\n",
    "    call_number_table_id = \"call_numbers_staging\"\n",
    "    contributors_table_id = \"contributors_staging\"\n",
    "    subjects_table_id = \"subjects_staging\"\n",
    "    notes_table_id = \"notes_staging\"\n",
    "\n",
    "    # Grab a blob from the heap\n",
    "    first_blob = gcs_client.pop_blob(bucket_name, patterns_file)\n",
    "    if not first_blob:\n",
    "        return False\n",
    "\n",
    "    print(f\"Processing blob: {first_blob.name}\")\n",
    "\n",
    "    # Download to memory\n",
    "    blob_data = gcs_client.download_blob_to_memory(bucket_name, first_blob.name)\n",
    "    json_data = json.loads(blob_data)\n",
    "\n",
    "    results = json_data[\"results\"]\n",
    "\n",
    "    # Initialize lists to hold DataFrames\n",
    "    df_main_list = []\n",
    "    df_item_list = []\n",
    "    df_resources_list = []\n",
    "    df_call_number_list = []\n",
    "    df_contributors_list = []\n",
    "    df_subjects_list = []\n",
    "    df_notes_list = []\n",
    "\n",
    "    for result in results:\n",
    "        df_main_list.append(normalize_main(result))\n",
    "        df_item_list.append(normalize_item(result))\n",
    "        df_resources_list.append(normalize_resources(result))\n",
    "        df_call_number_list.append(normalize_call_numbers(result))\n",
    "        df_contributors_list.append(normalize_contributors(result))\n",
    "        df_subjects_list.append(normalize_subjects(result))\n",
    "        df_notes_list.append(normalize_notes(result))\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    df_main = pd.concat(df_main_list, ignore_index=True)\n",
    "    df_item = pd.concat(df_item_list, ignore_index=True)\n",
    "    df_resources = pd.concat(df_resources_list, ignore_index=True)\n",
    "    df_call_number = pd.concat(df_call_number_list, ignore_index=True)\n",
    "    df_contributors = pd.concat(df_contributors_list, ignore_index=True)\n",
    "    df_subjects = pd.concat(df_subjects_list, ignore_index=True)\n",
    "    df_notes = pd.concat(df_notes_list, ignore_index=True)\n",
    "\n",
    "    # Load DataFrames into BigQuery tables\n",
    "    # bq_client.load_dataframe_to_table(dataset_id, main_table_id, df_main)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, item_table_id, df_item)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, resources_table_id, df_resources)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, call_number_table_id, df_call_number)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, contributors_table_id, df_contributors)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, subjects_table_id, df_subjects)\n",
    "    bq_client.load_dataframe_to_table(dataset_id, notes_table_id, df_notes)\n",
    "\n",
    "    # Move the blob to the processed_results bucket\n",
    "    gcs_client.copy_blob(bucket_name, first_blob.name, processed_bucket_name, first_blob.name)\n",
    "    gcs_client.delete_blob(bucket_name, first_blob.name)\n",
    "    print(f\"Blob {first_blob.name} moved to {processed_bucket_name} and deleted from {bucket_name}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Run the script locally or in the cloud.')\n",
    "    parser.add_argument('--local', action='store_true', help='Run the script locally with credentials path')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dataset_id = \"supreme_court\"\n",
    "    patterns_file = os.getenv('PATTERNS_FILE', 'exclude.txt')\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'smart-axis-421517')\n",
    "    bucket_name = os.getenv('BUCKET_NAME', 'loc-scraper')\n",
    "    processed_bucket_name = \"processed_results\"\n",
    "\n",
    "    credentials_path = None\n",
    "    if args.local:\n",
    "        credentials_path = os.getenv('GCP_CREDENTIALS_PATH', 'secret.json')\n",
    "\n",
    "    # Initialize logging\n",
    "    logging_client = initialize_google_cloud_logging_client(project_id, credentials_path)\n",
    "    logging_client.setup_logging()\n",
    "\n",
    "    # List Buckets for testing\n",
    "    gcs_client = initialize_gcs_client(project_id, credentials_path)\n",
    "    list_gcs_buckets(gcs_client)\n",
    "\n",
    "    # Create the processed_results bucket if not exists\n",
    "    # gcs_client.create_bucket(processed_bucket_name)\n",
    "\n",
    "    bq_client = initialize_bq_client(project_id, credentials_path)\n",
    "\n",
    "    # Create the dataset if not exists\n",
    "    bq_client.create_dataset(dataset_id)\n",
    "\n",
    "    # Create tables and schemas\n",
    "    create_tables_and_schemas(bq_client, bucket_name, patterns_file, gcs_client, dataset_id)\n",
    "    # def create_tables_and_schemas(bq_client, bucket_name, patterns_file, gcs_client, dataset_id):\n",
    "\n",
    "    # Process blobs in a loop\n",
    "    while process_blob(gcs_client, bq_client, bucket_name, processed_bucket_name, patterns_file, dataset_id):\n",
    "        print(\"Processed a blob, checking for more...\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6076bf3-5336-43b5-9082-13aad7ff4e90",
   "metadata": {},
   "source": [
    "### GCP Cloud Run \n",
    "\n",
    "I want this to run autonomously for me on GCP\n",
    "\n",
    "To do this I will need to \n",
    "\n",
    "1. Create a DockerFile\n",
    "2. Build the image on gcp\n",
    "3. create a job to run it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad827c-f0b2-4416-8354-d1249bb81c25",
   "metadata": {},
   "source": [
    "### Create the DockerFile\n",
    "\n",
    "My Dockerfile looks something like this. Ignore the quickstart code thta i've commente dout. I use that as a reference.\n",
    "\n",
    "[GH Link](https://raw.githubusercontent.com/justin-napolitano/loc_normalizer/main/Dockerfile)\n",
    "\n",
    "\n",
    "```yaml\n",
    "# # Use the Alpine Linux base image\n",
    "# FROM alpine:latest\n",
    "\n",
    "# # Set the working directory inside the container\n",
    "# WORKDIR /app\n",
    "\n",
    "# # Copy a simple script that prints \"Hello, World!\" into the container\n",
    "# COPY /src/hello.sh .\n",
    "\n",
    "# # Make the script executable\n",
    "# RUN chmod +x hello.sh\n",
    "\n",
    "# # Define the command to run when the container starts\n",
    "# CMD [\"./hello.sh\"]\n",
    "\n",
    "\n",
    "# Use the official Python image from Docker Hub\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY ./src /app\n",
    "COPY requirements.txt /app\n",
    "\n",
    "# Install any needed dependencies specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Run the Python script when the container launches\n",
    "CMD [\"python\", \"loc_scraper.py\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68ec6d-7441-42c9-ab47-7c2e214046ab",
   "metadata": {},
   "source": [
    "#### Using Cloudbuild\n",
    "\n",
    "I want to automate the entire build and deploy process by passing the steps to google's cloud build service.  \n",
    "\n",
    "My [file](https://github.com/justin-napolitano/loc_normalizer/blob/main/src/cloudbuild.yaml) looks like this...\n",
    "\n",
    "```yaml\n",
    "\n",
    "steps:\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['build', '-t', 'gcr.io/$PROJECT_ID/$IMAGE_NAME', '.']\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    args: ['push', 'gcr.io/$PROJECT_ID/$IMAGE_NAME']\n",
    "  - name: 'gcr.io/cloud-builders/gcloud'\n",
    "    args: ['run', 'deploy', '$SERVICE_NAME',\n",
    "           '--image', 'gcr.io/$PROJECT_ID/$IMAGE_NAME',\n",
    "           '--platform', 'managed',\n",
    "           '--region', '$REGION',\n",
    "           '--allow-unauthenticated']\n",
    "\n",
    "substitutions:\n",
    "  _PROJECT_ID: 'smart-axis-421517'\n",
    "  _IMAGE_NAME: 'loc-flattener-image'\n",
    "  _SERVICE_NAME: 'loc-flattener'\n",
    "  _REGION: 'us-west2'  # e.g., us-central1\n",
    "\n",
    "timeout: '1200s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf318d-fa5e-4978-8461-a53d65cb0893",
   "metadata": {},
   "source": [
    "#### Submit the build\n",
    "\n",
    "To sbumit the build run the following from the cli or save to as script.\n",
    "\n",
    "```bash\n",
    "gcloud builds submit --config cloudbuild.yaml .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b243f-ecda-4bbe-93f6-62de5605b5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
